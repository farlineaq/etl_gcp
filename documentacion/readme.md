# PROYECTO FUNNEL

**√Årea:** 

**Analista √Ågil:** Sara Pena Restrepo - Edwin Santiago Gonzalez

**Dominio:** 

**Proyecto:** FUNNEL

**Palabras Clave:** Pipeline, Google Cloud Platform (GCP), Arquitectura de datos, BigQuery, PySpark, Delta Lake, Cloud Logging, IAM (Identity and Access Management),

**Infraestructura de despliegue:** Google Cloud Platform (GCP)

**Sistemas Origen:** Integraci√≥n manual y a trav√©s de Terraform

**Sistemas Destino:** Cloud Storage - BigQuery

**Tipo desarrollo:** 

**Versi√≥n Lenguaje:** Python

#### Tabla de contenido

- [Descripci√≥n de la necesidad](#descripci√≥n-de-la-necesidad)
- [Diagrama de la necesidad](#diagrama-de-la-necesidad)
- [Clasificacion de las Interfaces](#clasificacion-de-las-interfaces)
- [Atributos de calidad de la solucion](#atributos-de-calidad-de-la-solucion)
- [Diagrama de componentes de la Interfaz](#diagrama-de-componentes-de-la-interfaz)
- [Consideraciones](#consideraciones)
- [Mapeo de datos](#mapeo-de-datos)
  - [Mapeo Movil_Exito_Bolsillo](#mapeo_movil_exito_bolsillo)
  - [Mapeo Movil_Exito_Gestores](#mapeo_movil_exito_gestores)
  - [Mapeo Movil_Exito_Tipoajuste](#mapeo_movil_exito_tipoajuste)
- [Caracter√≠sticas t√©cnicas de la Interfaz](#caracter√≠sticas-t√©cnicas-de-la-interfaz)
- [Manejo de Errores](#manejo-de-errores)
- [Manejo de reproceso](#manejo-de-reproceso)
- [Manual de despliegue](#manual-de-despliegue)

* [Inventario de Artefactos](#inventario-de-artefactos)
* [Topolog√≠as](#topolog√≠as) 
* [Directorios](#directorios)
* [Operaciones de la Interfaz (Servicio)](#Operaciones-de-la-Interfaz-(Servicio))


#### Tabla de contenido

- [Introducci√≥n](#introducci√≥n)
- [Descripci√≥n de la necesidad](#descripci√≥n-de-la-necesidad)
  - [Contexto](#contexto)
  - [Objetivo](#objetivo)
  - [Uso de los datos](#uso-de-los-datos)
  - [Requerimientos funcionales](#requerimientos-funcionales)
  - [Alcance y resultados esperados](#alcance-y-resultados-esperados)
    - [Incluye](#incluye)
    - [No incluye](#no-incluye)
- [Arquitectura](#arquitectura)
  - [Diagrama f√≠sico de la necesidad](#diagrama-f√≠sico-de-la-necesidad)
  - [Arquitectura de integraciones](#arquitectura-de-integraciones)
  - [Arquitectura de datos](#arquitectura-de-datos)
  - [Beneficios de la arquitectura](#beneficios-de-la-arquitectura)
  - [Diagrama de componentes de la soluci√≥n](#diagrama-de-componentes-de-la-soluci√≥n)
  - [Consideraciones](#consideraciones)
    - [De arquitectura](#de-arquitectura)
    - [T√©cnicas](#t√©cnicas)
  - [Caracter√≠sticas t√©cnicas de la soluci√≥n](#caracter√≠sticas-t√©cnicas-de-la-soluci√≥n)
  - [Manejo de errores](#manejo-de-errores)
  - [Manejo de reproceso](#manejo-de-reproceso)
- [Gobierno de datos](#gobierno-de-datos)
  - [Tecnolog√≠a](#tecnolog√≠a)
    - [Almacenamiento de datos](#almacenamiento-de-datos)
    - [Procesamiento](#procesamiento)
    - [Consumo](#consumo)
    - [Monitoreo](#monitoreo)
    - [Seguridad y privacidad](#seguridad-y-privacidad)
  - [Procesos](#procesos)
    - [Arquitectura](#arquitectura-1)
    - [Linaje de datos](#linaje-de-datos)
    - [Ciclo de vida de los datos](#ciclo-de-vida-de-los-datos)
    - [Monitorizaci√≥n y gesti√≥n](#monitorizaci√≥n-y-gesti√≥n)
    - [Soporte y mantenimiento](#soporte-y-mantenimiento)
  - [Personas](#personas)
    - [Esquema jer√°rquico de Google Cloud Platform](#esquema-jer√°rquico-de-google-cloud-platform)
    - [Pol√≠ticas de permisos](#pol√≠ticas-de-permisos)
    - [Asignaci√≥n de roles](#asignaci√≥n-de-roles)
    - [**Cuentas de servicios**](#cuentas-de-servicios)


# Introducci√≥n

En un mundo impulsado por la revoluci√≥n tecnol√≥gica, la gesti√≥n efectiva de datos se ha convertido en un pilar fundamental para el √©xito empresarial.
En este sentido, Grupo √âxito identifica la necesidad de adaptarse a las demandas cambiantes del entorno empresarial para que por medio del proyecto "Funnel de Mercadeo" implemente en la nube de Google Cloud Platform (GCP) 
una soluci√≥n que permita la integraci√≥n de datos con el objetivo no solo de optimizar operaciones de consulta diarias, sino tambi√©n, para establecer bases s√≥lidas al rededor de una visi√≥n estrat√©gica a largo plazo.
Lo anterior, con el fin de garantizar la disponibilidad y calidad de la informaci√≥n, para as√≠, poder tomar mejores decisiones estrat√©gicas basadas en datos confiables y oportunos.

# Descripci√≥n de la necesidad

## Contexto

El escenario empresarial cada vez es m√°s din√°mico y competitivo, la capacidad de adaptaci√≥n y la toma de decisiones fundamentadas en datos son cruciales para el √©xito.
Grupo √âxito reconoce esta realidad y busca posicionarse a la vanguardia como negocio a traves del proyecto "Funnel de Mercadeo" en el que por medio de la gesti√≥n de datos, busca mejorar la toma de decisiones estrat√©gicas y operativas.

## Objetivo
El objetivo del proyecto es implementar una soluci√≥n integral en la nube de Google Cloud Platform (GCP) a traves de una arquitectura de datos para recopilar, procesar, almacenar, consumir, gobernar y gestionar los datos provenientes de diferentes fuentes de informaci√≥n.
Posteriormente, el proyecto busca establecer y desarrollar estrategias de analytica confiables por medio de indicadores. Esto, se garantiza por la construcci√≥n de un modelo dimensional que asegura la disponibilidad y calidad de la informaci√≥n para responder a las consultas y requerimientos necesarios de manera optima y costo-eficiente.

## Uso de los datos
La data disponible ser√° utilizada para disponibilizar y construir 19 indicadores clave de rendimiento (KPIs). 
Estos KPIs no solo servir√°n para representar insights confiables de la compa√±ia, sino que tambi√©n funcionaran como herramientas fundamentales para desarrollar estrategias de analisis y soportar la toma de decisiones efectivas.

## Requerimientos funcionales
**Arquitectura y Dise√±os de Flujos de Integraci√≥n** 
  - Se requiere que la arquitectura dise√±ada y los flujos de integraci√≥n, permitan la disposici√≥n de los datos en las siguientes capas: Raw Trusted y Refined.

**Procesamiento de los Datos**
  - Se requiere de la recopilaci√≥n de datos provenientes de diferentes fuentes de informaci√≥n y que se disponibilicen correctamente en la capa Raw.
  - Se requiere del adecuado procesamiento de datos para garantizar la calidad de la informaci√≥n en la capa Trusted.
  - Se requiere que los datos sean gestionados de manera segura a lo largo de su ciclo de vida.

**Modelo Dimensional**
  - Se requiere de la construcci√≥n de un modelo dimensional que permita la disponibilidad y calidad de la informaci√≥n en la capa Refined. 
  - Se requiere que los datos sean gestionados de manera eficiente para garantizar un rendimiento √≥ptimo y organizaci√≥n de los datos en la plataforma.
  
**Consumo de datos:**
  - Se requiere que los datos generados por el pipeline esten disponibles para consulta, an√°lisis y construcci√≥n de los indicadores.

**Monitoreo y Seguridad de datos:**
  - Se requiere de una estrategia de monitoreo, seguridad y control.
  - Se requiere que la estrategia incluya la supervisi√≥n de los flujos de datos, el rendimiento de la soluci√≥n, la privacidad de los datos y la detecci√≥n proactiva de posibles problemas.

**Gobernanza de datos:**
  - Se requiere la gobernanza de datos para garantizar la calidad, integridad, seguridad y utilizaci√≥n efectiva de los datos en una organizaci√≥n.
  - Se requiere definir pol√≠ticas de gesti√≥n de identidades y accesos que regulen qui√©nes tienen permisos para acceder, modificar y gestionar los datos.
  - Se requiere que la estrategia incluya la implementaci√≥n de pol√≠ticas para la gesti√≥n de personas y procesos.
  - Se requiere definir pol√≠ticas de gesti√≥n de identidades y accesos con el objetivo de regular qui√©nes tienen permisos para acceder, modificar y gestionar los datos.

**Documentaci√≥n y Despliegue:**
  - Se requiere que la soluci√≥n cuente con diagramas de proceso y linaje de datos claramente documentados de manera que proporcionen una visi√≥n completa de c√≥mo los datos fluyen a trav√©s de la soluci√≥n.
  - Se requiere de documentaci√≥n al rededor del modelo dimensional y el informe de los indicadores claves de desempe√±o para la comprensi√≥n del sistema.

## Alcance y resultados esperados

### Incluye

- Entendimiento de las integraciones y lineamientos de arquitectura de datos de Grupo √âxito.
- Dise√±o e implementaci√≥n de los flujos de integraci√≥n.
- Dise√±o, implementaci√≥n y documentaci√≥n sobre las definiciones de arquitectura de datos.
- Dise√±o e implementaci√≥n y documentaci√≥n del modelo dimensional adaptado a los requerimientos del negocio.
- Creaci√≥n de las vistas y tablas en BigQuery (capa trusted).
- Creaci√≥n de stored procedures para la generaci√≥n de KPis (capa refined).
- Informe de Indicadores Clave de Desempe√±o (KPIs)
- Dise√±o e implementaci√≥n de estrategias de monitoreo y control de la soluci√≥n.
- Dise√±o e implementaci√≥n de pol√≠ticas de gesti√≥n de identidades y accesos.
- Dise√±o e implementaci√≥n de diagramas de proceso y linaje de datos.
- Despliegue a los ambientes que El √âxito disponga (DEV y PDN).

### No incluye

- Construcci√≥n de tableros de visualizaci√≥n de datos.

# Arquitectura

En el contexto de una transformaci√≥n radical impulsada por las tecnolog√≠as en la nube, Google Cloud Platform (GCP) se destaca como una soluci√≥n l√≠der, proporcionando servicios innovadores que ofrecen agilidad, escalabilidad y seguridad esenciales para las empresas modernas. 
Este apartado hace enfasis en la arquitectura de datos propuesta para el proyecto Funnel de Mercadeo, destacando la necesidad de una infraestructura bien planificada capaz de respaldar tanto la operaci√≥n inmediata como la visi√≥n a largo plazo de la organizaci√≥n.

De manera general, se subraya la importancia de la arquitectura de datos en la gesti√≥n eficiente de la informaci√≥n empresarial. Este conjunto de estructuras y procesos se encarga de capturar, procesar, almacenar, modelar, publicar y aprovechar los datos de manera efectiva. 
En el proyecto actual, se enfoca en la construcci√≥n de un Data Lakehouse en Google Cloud Platform, consolidando datos desde diversas fuentes para su almacenamiento centralizado. 

A continuaci√≥n, se anexa el documento correspondiente a la arquitectura implementada para el proyecto FUNNEL.
El documento sirve como un detallado insumo fundamental que proporciona el marco, el dise√±o sistem√°tico e implementaci√≥n de la arquitectura, siguiendo las mejores pr√°cticas y recomendaciones de expertos. 
El objetivo es establecer una base robusta y eficiente para el manejo de datos, potenciando la inteligencia de negocios y la toma de decisiones informadas.

![Documentaci√≥n Detallada de la Arquitectura]()

## Consideraciones

### T√©cnicas

- Es importante tener en cuenta que los cambios en los flujos de datos (desarrollos t√©cnicos) pueden tener un impacto significativo en el sistema. Por esta raz√≥n, se han establecido procesos rigurosos para asegurar la estabilidad y calidad de los datos, en lugar de considerar modificaciones a los flujos. Este enfoque cuidadoso contribuir√° a una gesti√≥n m√°s eficiente y efectiva de los datos a largo plazo.
- La tolerancia a fallos de la aplicaci√≥n est√° contemplada de la siguiente manera: 
  - Se cuenta con una zona de cuarentena a donde se llevan los archivos que no pasen el proceso de validaci√≥n. 
  - El sistema esta habilitado un punto de control que almacena el estado del procesamiento (checkpoint), si en alg√∫n momento falla la aplicaci√≥n, el procesamiento se ejecuta desde el √∫ltimo punto de control correcto almacenado.
- En Spark, se aplica el esquema "on read". Esto significa que se asigna el esquema cuando se leen los datos. La aplicaci√≥n del esquema en el momento de la lectura permite verificar autom√°ticamente la consistencia de los datos con el esquema especificado y, en caso de ser necesario, realizar una conversi√≥n de tipos. En caso de que el esquema del archivo de datos no coincida con el esquema definido, el archivo de datos es llevado a la zona de cuarentena para evitar afectar la integridad y calidad del dato.
- Esta aplicaci√≥n fue dise√±ada en Apache Spark 3.3.0 y Python 3.10. Adem√°s, se tienen las siguientes dependencias:
  - `py4j == 0.10.9.5`
  - `dynaconf == 3.2.2`
  - `toml == 0.10.2`
  - `great-expectations == 0.18.7`
  - `gcsfs == 2023.5.0`
- La aplicaci√≥n se dise√±√≥ usando la API de Structured Streaming de Spark, pero de tal forma que los ETLs se comporten como un proceso batch, esto usando un trigger de `once`. El objetivo es que, en lugar de procesar los datos en tiempo real, se procesen todos los datos disponibles en un solo lote. Esto se hace para aprovechar la funcionalidad de `checkpoint` que proporciona la API de Structured Streaming. El checkpointing es una funcionalidad clave que permite a Spark mantener un registro del estado actual del procesamiento de datos en un stream, incluyendo los datos que han sido procesados y los que a√∫n deben ser procesados. Esto es importante porque permite a Spark recuperarse de fallos y continuar el procesamiento de datos desde donde lo dej√≥ en caso de un reinicio o una interrupci√≥n.

## Caracter√≠sticas t√©cnicas de la soluci√≥n

Las hojas t√©cnicas de infraestructura se relacionan en las siguientes tablas:

| Sistema                                     | Tipo             | Nombre del recurso                                         |
|---------------------------------------------|------------------|------------------------------------------------------------|   
| GCS (Capa Raw)                              | Almacenamiento   | co-grupo-exito-funnel-mercd-raw-data-[ambiente]            |
| Dataproc Workflow (Carga Inicial Day)       | Flujo de trabajo | dp-funnel-mercd-workflow-carga-inicial-day-[ambiente]      |
| Dataproc Workflow (Carga Inicial Month)     | Flujo de trabajo | dp-funnel-mercd-workflow-carga-inicial-month-[ambiente]    |
| Dataproc Workflow (Carga Inicial Year)      | Flujo de trabajo | dp-funnel-mercd-workflow-carga-inicial-year-[ambiente]     |
| Dataproc Workflow (Delta Day)               | Flujo de trabajo | dp-funnel-mercd-workflow-delta-day-[ambiente]              |
| Dataproc Workflow (Delta Month)             | Flujo de trabajo | dp-funnel-mercd-workflow-delta-month-[ambiente]            |
| Dataproc Workflow (Delta Year)              | Flujo de trabajo | dp-funnel-mercd-workflow-delta-year-[ambiente]             |
| Dataproc Workflow (Delta Duplicado Month)   | Flujo de trabajo | dp-funnel-mercd-workflow-delta-duplicado-month-[ambiente]  |
| Dataproc Cluster                            | Procesamiento    | dp-funnel-mercd-pipeline-[ambiente]                        |
| Dataproc Job                                | Trabajo          | dp-funnel-mercd-job-[ambiente]                             |
| GCS (Capa Trusted)                          | Almacenamiento   | co-grupo-exito-funnel-mercd-trusted-data-[ambiente]        |
| BigQuery (Dataset)(Modelo Dimensional)      | Base de datos    | refined_funnel_mercd_dimensional_model                     |
| BigQuery (External tables dataset)          | Base de datos    | refined_funnel_mercd_external_tables                       |
| BigQuery ( Views dataset)                   | Base de datos    | refined_funnel_mercd_views                                 |
| BigQuery (Stored Procedures)                | Base de datos    | refined_funnel_mercd_procedures                            |
| Cloud Scheduler Job (Delta Day)             | Orquestador      | cs-funnel-mercd-scheduler-delta-day-[ambiente]             |
| Cloud Scheduler Job (Delta Month)           | Orquestador      | cs-funnel-mercd-scheduler-delta-month-[ambiente]           |
| Cloud Scheduler Job (Delta Year)            | Orquestador      | cs-funnel-mercd-scheduler-delta-year-[ambiente]            |
| Cloud Scheduler Job (Delta Duplicado Month) | Orquestador      | cs-funnel-mercd-scheduler-delta-duplicado-month-[ambiente] |

Esta integraci√≥n est√° orquestada por el Cloud Scheduler de GCP:
(modificar para agregar los otros shceduler)
| Par√°metros Cloud Scheduler |                                                                                                                                                                        |
|----------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Nombre de la Tarea         |                                                                                                                                    |
| Frecuencia                 | 0 6 * * *                                                                                                                                                              |
| Objetivo                   | HTTP                                                                                                                                                                   |
| URL                        | |
| Verbo HTTP                 | POST                                                                                                                                                                   |
| Tipo de cuerpo             | JSON                                                                                                                                                                   |
| Cuerpo                     | {User-Agent: Google-Cloud-Scheduler}                                                                                                                                   |

## Manejo de errores

|                | Si/No | C√≥mo se realiza                                                          |
|----------------|-------|--------------------------------------------------------------------------|
| Notificaciones | Si    | Manejo a traves de operaciones con el equipo de monitoreo de Grupo Exito |
| Reintentos     | Si    | Se ejecuta de nuevo el flujo de trabajo manualmente                      |
| Trazabilidad   | Si    | En los logs enrutados al proyecto de logging y monitoreo                 |

## Manejo de reproceso

Especificar los puntos en que se debe hacer el reproceso y c√≥mo hacerlo.

| Punto                           | Como se reprocesa                                                     | Aplicabilidad |
|---------------------------------|-----------------------------------------------------------------------|---------------|
| Fallo en el trabajo de Dataproc | Se debe iniciar manualmente el trabajo en Dataproc desde el Pipeline. |               |

# Gobierno de datos

El gobierno de datos es el conjunto de pol√≠ticas, procedimientos y pr√°cticas que se utilizan para gestionar, proteger y utilizar los datos de una organizaci√≥n. Esto incluye aspectos como la recolecci√≥n, almacenamiento, uso, seguridad y privacidad de los datos, as√≠ como la toma de decisiones basadas en datos. El objetivo del gobierno de datos es asegurar que los datos se utilicen de manera efectiva y √©tica, y que se cumplan las regulaciones y leyes aplicables.

En este proyecto se siguen las buenas pr√°cticas recomendadas por **DAMA** (Data Management Association), que es una asociaci√≥n profesional que promueve la gesti√≥n de datos y el gobierno de datos. El marco de trabajo de **DAMA** se conoce como el marco de referencia de gobierno de datos de **DAMA (DAMA-DMBOK)**. Este marco proporciona una estructura general para la gesti√≥n de los datos de una organizaci√≥n, incluyendo los procesos, roles y responsabilidades, y las mejores pr√°cticas para la gesti√≥n de datos.

**DAMA-DMBOK** se basa en 3 pilares fundamentales: personas, procesos y tecnolog√≠a.

1. **Tecnolog√≠a**: Es el uso de la tecnolog√≠a para apoyar la gesti√≥n de los datos. Incluye la selecci√≥n, implementaci√≥n, integraci√≥n con los procesos de negocio y la gesti√≥n de la tecnolog√≠a.
2. **Procesos**: Es la estructura de los procesos que se utilizan para recopilar, almacenar, gestionar y proteger los datos. Incluye la definici√≥n, documentaci√≥n, medida de la eficacia y la mejora continua de los procesos.
3. **Personas**: Es el aspecto humano de la gesti√≥n de los datos. Incluye la identificaci√≥n de los roles y responsabilidades, la formaci√≥n y el desarrollo de habilidades, y la gesti√≥n de los cambios culturales necesarios para la implementaci√≥n exitosa de una estrategia de gobierno de datos.

Cada uno de estos pilares son importante para el √©xito de la estrategia de gobierno de datos, ya que se constituye en un engranaje funcional para garantizar que los datos sean precisos, completos y est√©n disponibles para su uso.

## Tecnolog√≠a

El pilar de tecnolog√≠a del **DAMA-DMBOK** es un componente importante del marco de gobierno de datos que se refiere a la selecci√≥n, implementaci√≥n y uso de tecnolog√≠as de informaci√≥n para apoyar la gesti√≥n de los datos.

Esto incluye la selecci√≥n de herramientas y plataformas que ayuden a capturar, almacenar, procesar, distribuir y analizar los datos de manera efectiva. Tambi√©n incluye la implementaci√≥n de medidas de seguridad y privacidad para garantizar la protecci√≥n de los datos, as√≠ como la implementaci√≥n de soluciones de gesti√≥n de metadatos y de calidad de datos para garantizar la integridad y la consistencia de los datos a lo largo del tiempo.

### Almacenamiento de datos

**Cloud Storage**

Google Cloud Storage es un servicio de almacenamiento en la nube que ofrece Google. El servicio ofrece seguridad y rendimiento de alto nivel para almacenar grandes cantidades de datos sin tener que preocuparse por la administraci√≥n de la infraestructura subyacente. Es el almac√©n donde se alojan todos los datos del proyecto.

### Procesamiento

**Dataproc**

Google Cloud Dataproc es un servicio de procesamiento de datos en la nube de Google que simplifica la configuraci√≥n, el aprovisionamiento y el escalado de clusters de Apache Hadoop y Apache Spark para la computaci√≥n de datos. Es la herramienta encargada de ejecutar la aplicaci√≥n de Pyspark del proyecto.

**Apache Spark**

Apache Spark es un framework de procesamiento de datos distribuido que se utiliza com√∫nmente para construir procesos ETL (extracci√≥n, transformaci√≥n y carga) debido a su capacidad para manejar grandes vol√∫menes de datos de manera eficiente.

El funcionamiento de Spark para construir ETLs se basa en su capacidad para leer datos de diferentes fuentes, procesarlos y escribirlos en una variedad de formatos de salida.

### Consumo

**BigQuery**

BigQuery es un servicio de an√°lisis de datos en la nube de Google Cloud que permite a los usuarios realizar consultas a grandes vol√∫menes de datos en tiempo real. Es una soluci√≥n escalable y altamente disponible que se integra con otros productos de Google Cloud, como Google Storage, lo que facilita el an√°lisis a gran escala. BigQuery es una plataforma popular para an√°lisis de datos, data warehousing y Business Intelligence.

En este proyecto, BigQuery act√∫a como una capa de consumo de datos, lo que significa que no es necesario almacenarlos de manera nativa. Cuando un usuario realiza una consulta en BigQuery, los datos se leen directamente desde la fuente (en este caso, Cloud Storage) y se procesan en nodos de procesamiento distribuidos en la nube de Google. Estos nodos realizan el trabajo de consulta y env√≠an los resultados de vuelta al usuario.

### Monitoreo

**Cloud Monitoring**

Cloud Monitoring es un servicio de Google Cloud Platform (GCP) que permite a los usuarios supervisar y analizar el rendimiento de sus aplicaciones y recursos en GCP. Con Cloud Monitoring, los usuarios pueden recopilar datos de m√©tricas, registros y trazas, y utilizar estos datos para obtener una visi√≥n en tiempo real del rendimiento de sus aplicaciones y recursos. Esta herramienta se usa para generar las alertas automatizadas 4asociadas al procesamiento de datos.

**Cloud Logging**

Cloud Logging es un servicio de Google Cloud Platform (GCP) que permite a los usuarios recolectar, buscar, analizar y alertar sobre el uso de sus aplicaciones y sistemas en GCP. Es una herramienta clave para el monitoreo y la soluci√≥n de problemas en GCP.

Con Cloud Logging, los usuarios pueden recolectar registros de diferentes fuentes, como aplicaciones, contenedores, instancias de m√°quinas virtuales y dispositivos, y almacenarlos en un √∫nico lugar. Los registros pueden ser buscados y filtrados utilizando un lenguaje de consulta avanzado.

Es la herramienta que recibe los registros provenientes de los trabajos ejecutados en Dataproc.

### Seguridad y privacidad

**IAM**

Cloud IAM es un sistema de identidad y acceso de Google Cloud Platform (GCP) que controla qui√©n tiene acceso a los recursos de la nube y qu√© acciones pueden realizar con ellos. Con Cloud IAM, los usuarios y las aplicaciones pueden obtener acceso a los recursos de GCP de manera segura y controlada.

## Procesos

### Modelo Dimensional (darle un apartado en el doc general y aqui solamente hacer menci√≥n del uso dentro del negocio)

El modelo dimensional es una t√©cnica de modelado de datos que se utiliza com√∫nmente en el dise√±o de Data Warehouse y Data Marts. Se basa en tipos de tablas que principalmente hacen referencia a hechos y dimensiones. 
Los hechos son las m√©tricas que se desean analizar, mientras que las dimensiones son las categor√≠as que se utilizan para analizar los hechos. El modelo dimensional es una forma de organizar los datos que facilita la consulta y el an√°lisis de los datos.

#### Marco de integraci√≥n (Enfoque en el Negocio) (Ponerlo en introduccion 314 330 resumir en un parrafo)

Permite que los estrategas de datos, los profesionales en desarrollo del producto/servicio y las partes interesadas del negocio, puedan colaborar en la definici√≥n de los datos, la gesti√≥n de los datos y la toma de decisiones.

El objetivo es que haya una centralizaci√≥n para la toma de decisiones acertadas en respuesta a la necesidad del negocio, el dise√±o de la soluci√≥n y su funcionamiento, en este sentido surgen mejores respuestas a cuestionamientos tales como:
- C√≥mo administrar los datos
- C√≥mo obtener valor de los datos.
- C√≥mo minimizar costos de almacenamiento, procesamiento y rendimiento.
- C√≥mo reducir la complejidad de la infraestructura.
- C√≥mo administrar riesgos y garantizar el funcionamiento de la soluci√≥n.
- C√≥mo garantizar la escalabilidad y la disponibilidad de la soluci√≥n.
- C√≥mo establecer normativas ante requisitos legales, regulatorios y otros

##### Procesos (aqui se aplican los lineamientos de popliticas de estandares y controles)

Los procesos de gobierno constan de toda la documentaci√≥n recomendada en DAMA-DMBOK; sin embargo, a continuaci√≥n se comentan los procesos recomendados por "DGI Data Governance Framework" 
para la integraci√≥n de otras partes del negocio que se relacionan con el entendimiento, aplicabilidad y aprovechamiento de una soluci√≥n que cuente con el desarrollo de un modelo dimensional.

- Pol√≠ticas, Est√°ndares y controles  
- Matriz de escalamiento decisiones(quien administra esta parte)
- Requisitos de calidad de datos
- Roles y monitoreo
- Stakholders: - Interesados del negocio y objetivos sobre los datos. Pueden provenir de adentro y fuera de la organizaci√≥n, son los consumidores de los datos, establecen las reglas y los objetivos.

### Arquitectura

En el [apartado 3 del documento](https://dev.azure.com/grupo-exito/eaff7c2c-ee42-4b16-abe8-670b3fb8b200/_apis/git/repositories/8fdc11ad-308b-465b-bab3-884a5269a145/items?path=/docs/img/Diagrama%20de%20Despliegue.png&versionDescriptor%5BversionOptions%5D=0&versionDescriptor%5BversionType%5D=0&versionDescriptor%5Bversion%5D=feature/mateomg&resolveLfs=true&%24format=octetStream&api-version=5.0),
relacionado con la arquitectura de integraci√≥n y datos, se detallan las definiciones e interacciones entre las diferentes herramientas utilizadas en el proyecto.
          
### Linaje de datos

Linaje de datos es el proceso de rastrear la historia de los datos desde su origen hasta su uso actual o futuro. Es una forma de rastrear y documentar la evoluci√≥n de los datos en un sistema, incluyendo cambios, transformaciones y usos. El linaje de datos proporciona informaci√≥n valiosa sobre la calidad de los datos, su precisi√≥n y su confiabilidad.

El linaje de los datos se divide en dos categor√≠as: linaje l√≥gico y linaje f√≠sico. El linaje l√≥gico se refiere al camino que sigue el dato a trav√©s de los procesos y sistemas de negocio, mientras que el linaje f√≠sico se refiere al camino que sigue el dato a trav√©s de los sistemas de almacenamiento y bases de datos.

**Nota**: El linaje se relaciona en las siguientes p√°gina: 
- [Linaje de Datos Correspondiente a la Capa Trusted](linaje_trusted.md)    
- [Linaje de Datos Correspondiente a la Capa Refined](linaje_refined.md)

### Ciclo de vida de los datos (aclarar que para el proyecto no aplica)

El ciclo de vida de los datos se refiere a las diferentes etapas que atraviesan los datos desde su creaci√≥n hasta su eliminaci√≥n o archivo.

1. **Fuente de datos**: Es la primera etapa en la que se generan o recolectan los datos.
2. **Captura**: Es la etapa en la que los datos se registran, se verifica la precisi√≥n de los datos, se validan y se limpian.
3. **Almacenamiento**: Es la etapa en la que los datos se guardan en un lugar seguro, accesible y confiable.
4. **Procesamiento**: Es la etapa en la que los datos se analizan, se transforman y se utilizan para generar informes y tomar decisiones.
5. **Uso**: Es la etapa en la que los datos se utilizan para generar informes, tomar decisiones y apoyar las operaciones de negocio.

![Diagrama ciclo de vida](https://dev.azure.com/grupo-exito/eaff7c2c-ee42-4b16-abe8-670b3fb8b200/_apis/git/repositories/8fdc11ad-308b-465b-bab3-884a5269a145/items?path=/docs/img/Ciclo_de_Vida.png&versionDescriptor%5BversionOptions%5D=0&versionDescriptor%5BversionType%5D=0&versionDescriptor%5Bversion%5D=feature/mateomg&resolveLfs=true&%24format=octetStream&api-version=5.0)

### Monitorizaci√≥n y Gesti√≥n (se le delega todo esto a operaciones - Arus, disponibilizar o decirles que buscar manual de errores como rastrear y que hacer respecto a eso)

La monitorizaci√≥n y gesti√≥n de la soluci√≥n en la nube est√° a cargo de los administradores de la soluci√≥n, en este caso, **ARUS**, con previa capacitaci√≥n por parte del equipo de Quind. Se tiene que todo el monitoreo y gesti√≥n de la soluci√≥n se hace desde un proyecto de GCP dedicado para este fin. Todos los registros asociados a la soluci√≥n se env√≠an a un topic Pub/Sub que es posteriormente consumido por los administradores de la soluci√≥n.

Los registros enviados son los que cumplen en siguiente filtro:

```graphql
resource.type="cloud_dataproc_job"
```

Los registros de la soluci√≥n se dise√±aron para tener una estructura y severidad definida. Esto permite encontrarlos f√°cilmente y tener el contexto suficiente en caso de presentarse alg√∫n error conocido. El filtro para encontrar estos logs es el siguiente:

```graphql
jsonPayload.class="LoggerProvider" 
severity=ERROR 
resource.type="cloud_dataproc_job"
```

Este filtro permite tener suficiente informaci√≥n en caso de encontrarse con alg√∫n archivo corrupto en la capa raw:

Los posibles mensajes de error est√°n asociados a la validaci√≥n que se hace:

- `"they are not csv files"` ‚Üí Si el archivo no es un archivo csv.
- `"error while reading file"` ‚Üí Si el archivo est√° corrupto y no es legible.
- `"schema does not match"` ‚Üí Si el esquema del archivo no coincide con el esquema definido.

```json
[
   {
      "message":"This is the error message",
      "files":[
         "These are", 
				 "the files",
				 "with the same error"
      ]
   },
	 {
      "message":"This is another error message",
      "files":[
         "These are", 
				 "other files",
				 "with the same error"
      ]
   },
	 ...
]
```

**Comportamientos at√≠picos de la soluci√≥n**

A continuaci√≥n se detalla el plan de acci√≥n en caso de tener comportamientos at√≠picos:

| Comportamiento                          | Acci√≥n                                     |
|-----------------------------------------|--------------------------------------------|
| Crash o error en Dataproc               | Notificar y/o escalar al √°rea responsable. |

**Caracterizaci√≥n de eventos**

Esta tabla detalla la caracterizaci√≥n de eventos cr√≠ticos en el marco del monitoreo de la soluci√≥n. El objetivo es establecer claramente los umbrales de advertencia y cr√≠ticos, las frecuencias de chequeo, y las causas potenciales de cada evento. Adem√°s, proporciona un marco de acci√≥n y escalaci√≥n para garantizar una respuesta r√°pida y efectiva a cualquier evento que pueda afectar la integridad de los datos y el rendimiento del sistema.

| Id Tipo de servicio (Metrica)              | % Warning | % Critical | Id Tipo de umbral | Frecuencia de chequeo (Horas) | Causa del Evento             | Notificaci√≥n Alarma | Acci√≥n Cr√≠tica                            | Escalamiento 1 | Escalamiento 2 | Escalamiento 3 |
|--------------------------------------------|-----------|------------|-------------------|-------------------------------|------------------------------|---------------------|-------------------------------------------|----------------|----------------|----------------|
| ETL con errores                            | N/A       | N/A        | Error             | 12                            | `Error en los datos`         | Error en log        | Notificar al analista funcional           | Super usuario  | L√≠der t√©cnico  | Equipo Cloud   |
| Archivo no es csv                          | N/A       | N/A        | Error             | 12                            | `"they are not csv files"`   | Error en log        | Notificar y/o escalar al √°rea responsable | Super usuario  | L√≠der t√©cnico  | Equipo Cloud   |
| Archivo corrupto y no legible              | N/A       | N/A        | Error             | 12                            | `"error while reading file"` | Error en log        | Notificar y/o escalar al √°rea responsable | Super usuario  | L√≠der t√©cnico  | Equipo Cloud   |
| Esquema del archivo no coincide            | N/A       | N/A        | Error             | 12                            | `"schema does not match"`    | Error en log        | Notificar y/o escalar al √°rea responsable | Super usuario  | L√≠der t√©cnico  | Equipo Cloud   |
| Crash o error en Dataproc (Comportamiento) | N/A       | N/A        | Error             | 12                            | `Crash o error en Dataproc`  | Error en log        | Notificar y/o escalar al √°rea responsable | Super usuario  | L√≠der t√©cnico  | Equipo Cloud   |

### Soporte y mantenimiento

El soporte y mantenimiento de la soluci√≥n tambi√©n est√° a cargo de los administradores de la soluci√≥n, **ARUS**.

En GCP, el soporte y el mantenimiento son responsabilidad de Google. Esto significa que Google se encarga de la disponibilidad, escalabilidad y seguridad de la plataforma y los servicios, lo que permite a los clientes centrarse en el desarrollo y la implementaci√≥n de sus aplicaciones y servicios.

GCP ofrece diferentes niveles de soporte para adaptarse a las necesidades de los clientes, desde el soporte gratuito hasta el soporte premium con respuesta 24/7. Adem√°s, Google realiza actualizaciones y mejoras de forma regular para mantener la plataforma y los servicios al d√≠a y optimizados para un rendimiento √≥ptimo.

Sin embargo, los clientes son responsables de la administraci√≥n y el mantenimiento de sus aplicaciones y servicios en GCP, incluyendo la configuraci√≥n y el monitoreo de la seguridad. Para facilitar el mantenimiento, GCP ofrece herramientas y soluciones de gesti√≥n de aplicaciones, incluyendo la monitorizaci√≥n y el registro de eventos.

## Personas

La gesti√≥n de personas en Google Cloud Platform (GCP) se lleva a cabo usando el recurso Cloud IAM (Cloud Identity and Access Management). Cloud IAM es un servicio de seguridad de GCP que permite a los administradores controlar qui√©n tiene acceso a qu√© recursos y qu√© acciones pueden realizar en ellos, mediante la asignaci√≥n de roles, el control de acceso basado en contexto, la integraci√≥n con otras herramientas de seguridad y la auditor√≠a y registro de acceso.

Los roles en GCP se utilizan para otorgar permisos espec√≠ficos a los usuarios y grupos para acceder y administrar recursos en un proyecto o en una organizaci√≥n. Los roles incluyen tareas como verificar la facturaci√≥n, crear instancias de m√°quina virtual, administrar bases de datos, etc.

Los permisos en GCP se utilizan para controlar el acceso a los recursos individuales, como una instancia de m√°quina virtual o una base de datos. Los permisos se asignan a trav√©s de los roles y se pueden otorgar a usuarios y grupos espec√≠ficos.

Se sigue una pol√≠tica de ‚Äú`Who can do what on which resource‚Äù.`

- **Who:** un principal al cual se le van a asignar ciertos roles.
- **Can do what:** las acciones que se quiere que el principal tenga la capacidad de hacer.
- **On which resource**: con la acci√≥n definida se pueden constatar la serie de roles y permisos necesarios y a que nivel de la jerarqu√≠a se asignan estos.

### Esquema jer√°rquico de Google Cloud Platform

Los recursos de Google Cloud est√°n organizados jer√°rquicamente. Todos los recursos, excepto el recurso m√°s alto de una jerarqu√≠a, tienen exactamente un elemento superior. En el nivel m√°s bajo, los recursos de servicio son los componentes fundamentales que conforman todos los servicios de Google Cloud.

![jerarquia_cloud.png](https://dev.azure.com/grupo-exito/eaff7c2c-ee42-4b16-abe8-670b3fb8b200/_apis/git/repositories/8fdc11ad-308b-465b-bab3-884a5269a145/items?path=/documentacion/img/jerarquia_gcp.png&versionDescriptor%5BversionOptions%5D=0&versionDescriptor%5BversionType%5D=0&versionDescriptor%5Bversion%5D=develop&resolveLfs=true&%24format=octetStream&api-version=5.0)

IAM te permite establecer pol√≠ticas de permisos en los siguientes niveles de la jerarqu√≠a de recursos:

- **Nivel de organizaci√≥n:** El recurso de organizaci√≥n representa a tu empresa. Todos los recursos en la organizaci√≥n heredan las funciones de IAM otorgadas en este nivel. A fin de obtener m√°s informaci√≥n, consulta¬†[Control de acceso para organizaciones mediante IAM](https://cloud.google.com/resource-manager/docs/access-control-org?hl=es-419).
- **Nivel de carpeta:** Las carpetas pueden contener proyectos, otras carpetas o una combinaci√≥n de ambos. Los proyectos heredar√°n las funciones otorgadas en el nivel de carpeta m√°s alto, as√≠ como lo har√°n otras carpetas que se encuentren en esa carpeta superior. A fin de obtener m√°s informaci√≥n, consulta¬†[Control de acceso para carpetas mediante IAM](https://cloud.google.com/resource-manager/docs/access-control-folders?hl=es-419).
- **Nivel de proyecto:** Los proyectos representan un l√≠mite de confianza dentro de tu empresa. Los servicios dentro del mismo proyecto tienen un nivel de confianza predeterminado. Por ejemplo, las instancias de App¬†Engine pueden acceder a los dep√≥sitos de Cloud¬†Storage dentro del mismo proyecto. Los recursos dentro del proyecto heredan las funciones de IAM otorgadas en el nivel de proyecto. A fin de obtener m√°s informaci√≥n, consulta¬†[Control de acceso para proyectos mediante IAM](https://cloud.google.com/resource-manager/docs/access-control-proj?hl=es-419).
- **Nivel de recursos:** Adem√°s de los sistemas existentes de Cloud¬†Storage y LCA de BigQuery, los recursos adicionales, como los conjuntos de datos de Genomics, los temas de¬†Pub/Sub y las instancias de Compute¬†Engine, admiten funciones de nivel inferior para que puedas otorgar permisos a ciertos usuarios a un solo recurso dentro de un proyecto.

### Pol√≠ticas de permisos

En Identity¬†and¬†Access¬†Management (IAM), el acceso se otorga a trav√©s de¬†pol√≠ticas de permisos, tambi√©n conocidas como pol√≠ticas de IAM. Una pol√≠tica de permisos se adjunta a un recurso de Google¬†Cloud. Cada pol√≠tica de permisos contiene una colecci√≥n de¬†*vinculaciones de roles* ¬†que asocian una o m√°s principales, como usuarios o cuentas de servicio, a un rol de IAM. Estas vinculaciones de roles otorgan los roles especificados a las principales, tanto en el recurso al que se adjunta la pol√≠tica de permisos como en todos los¬†[elementos subordinados](https://cloud.google.com/resource-manager/docs/cloud-platform-resource-hierarchy?hl=es-419) de ese recurso. Para obtener m√°s informaci√≥n sobre las pol√≠ticas de permisos, consulta¬†[Comprende las pol√≠ticas de permisos](https://cloud.google.com/iam/docs/policies?hl=es-419).

### Asignaci√≥n de roles

Asignar roles a nivel de grupo en lugar de a nivel de miembro en Cloud IAM es considerada una buena pr√°ctica por varias razones:

- **Eficiencia:** Permite a los administradores gestionar el acceso de varios miembros a la vez, en lugar de hacerlo de forma individual. Esto puede ahorrar tiempo y esfuerzo a medida que se a√±aden o quitan miembros de la organizaci√≥n.
- **Flexibilidad:** Permite a los administradores cambiar el acceso de un miembro a varios recursos a la vez, en lugar de tener que hacerlo de forma individual. Esto puede ser √∫til cuando un miembro cambia de cargo o deja la organizaci√≥n.
- **Seguridad:** Permite a los administradores establecer pol√≠ticas de seguridad a nivel de grupo, lo que puede ayudar a prevenir errores y violaciones de seguridad.
- **Transparencia:** Permite a los administradores ver qu√© roles y permisos tienen los miembros de un grupo, lo que puede ayudar a identificar problemas de seguridad.

De acuerdo a estas definiciones, se identificaron 3 pol√≠ticas de permisos en el ambiente de producci√≥n:

- Ingeniero de Datos
- Cient√≠fico de Datos
- Analista de Datos

**Ingeniero de Datos**

![politica_ingeniero_de_datos.png](https://dev.azure.com/grupo-exito/eaff7c2c-ee42-4b16-abe8-670b3fb8b200/_apis/git/repositories/8fdc11ad-308b-465b-bab3-884a5269a145/items?path=/documentacion/img/politica_ingeniero_de_datos.png&versionDescriptor%5BversionOptions%5D=0&versionDescriptor%5BversionType%5D=0&versionDescriptor%5Bversion%5D=develop&resolveLfs=true&%24format=octetStream&api-version=5.0)

Pol√≠tica ingeniero de datos

| Rol                   | ID                         | Principal                                | Nivel en Jerarqu√≠a |
|-----------------------|----------------------------|------------------------------------------|--------------------|
| Viewer                | roles/viewer               | ingenierodedatosgcpdatex@grupo-exito.com | Proyecto           |
| Storage Object Viewer | roles/storage.objectViewer | ingenierodedatosgcpdatex@grupo-exito.com | Proyecto           |
| BigQuery Data Viewer  | roles/bigquery.dataViewer  | ingenierodedatosgcpdatex@grupo-exito.com | Proyecto           |


Esta pol√≠tica de permisos est√° dise√±ada para que el usuario tenga la posibilidad de visualizar todos los recursos asociados a un proyecto, sin poder modificar o crear nuevos recursos.

Esta pol√≠tica est√° asignada al grupo `ingenierodedatosgcpdatex@grupo-exito.com`.

**Cient√≠fico de Datos**

![politica_cientifico_de_datos.png](https://dev.azure.com/grupo-exito/eaff7c2c-ee42-4b16-abe8-670b3fb8b200/_apis/git/repositories/8fdc11ad-308b-465b-bab3-884a5269a145/items?path=/documentacion/img/politica_cientifico_de_datos.png&versionDescriptor%5BversionOptions%5D=0&versionDescriptor%5BversionType%5D=0&versionDescriptor%5Bversion%5D=develop&resolveLfs=true&%24format=octetStream&api-version=5.0)

Pol√≠tica cient√≠fico

| Title                      | Principal                                 | Nivel en Jerarqu√≠a | 
|----------------------------|-------------------------------------------|--------------------|
| Data Scientist Custom Role | cientificodedatosgcpdatex@grupo-exito.com | Proyecto           |

En el diagrama anterior podemos ver esquematizada la pol√≠tica de de permisos a nivel de proyecto. Se organizan los permisos de manera jer√°rquica y se agrupan en roles personalizados y predefinidos siguiendo la pol√≠tica de privilegios m√≠nimos. Los permisos asignados en una mayor jerarqu√≠a se heredan en los niveles de menor jerarqu√≠a. Por ejemplo, el permiso `bigquery.tables.get` otorgados a nivel de proyecto, se heredan a nivel de dataset.

Esta pol√≠tica de permisos permite que un usuario tenga las siguientes funciones y limitaciones:

- Consultar todas las vistas y las tablas externas de todos los dataset asociados al proyecto de **almacenamiento** desde un proyecto de **consulta** sin poder crear nuevas entidades ni modificar las existentes en BigQuery.
- Hacer trabajos de consultas en BigQuery desde un proyecto de **consulta**, sin poder guardar los resultados como vistas.
- Guardar y visualizar consultas SQL desde un proyecto de **consulta**.
- No puede acceder a la consola del proyecto de almacenamiento.

**Asignaci√≥n de pol√≠tica**

Esta pol√≠tica est√° asignada al grupo con email `cientificodedatosgcpdatex@grupo-exito.com`, por lo que para otorgar o denegar permisos a un usuario solo se debe agregar o eliminar el usuario del grupo en cuesti√≥n.

**Analista de Datos**

![politica_analista_de_datos.png](https://dev.azure.com/grupo-exito/eaff7c2c-ee42-4b16-abe8-670b3fb8b200/_apis/git/repositories/8fdc11ad-308b-465b-bab3-884a5269a145/items?path=/documentacion/img/politica_analista_de_datos.png&versionDescriptor%5BversionOptions%5D=0&versionDescriptor%5BversionType%5D=0&versionDescriptor%5Bversion%5D=develop&resolveLfs=true&%24format=octetStream&api-version=5.0)

Pol√≠tica analista de datos

| Title | Principal | Nivel en Jerarqu√≠a |
| --- | --- | --- |
| Data Analyst Custom Rol | analistadedatosgcpdatex@grupo-exito.com | dataset vistas | 
| Storage Object Viewer | analistadedatosgcpdatex@grupo-exito.com | bucket trusted | 

Se usan vistas autorizadas para dar acceso granular (columnar) a vistas espec√≠ficas. Una vista autorizada te permite compartir resultados de consultas con usuarios y grupos espec√≠ficos sin darles acceso a los datos de origen.

B√°sicamente el administrador crea una vista de una tabla espec√≠fica donde solo consulte las columnas y la informaci√≥n que el usuario tenga permitido desde negocio ver. Posteriormente, en la tabla de origen (Cloud Storage), esta nueva vista es autorizada a consultar la data necesaria y luego, en la nueva vista, se asignan los permisos necesarios al usuario para que pueda visualizarla y consultarla (esto se ve plasmado en el esquema de la pol√≠tica de permisos).

Puede encontrar m√°s informaci√≥n de este proceso en la p√°gina [Crear una vista autorizada](https://cloud.google.com/bigquery/docs/share-access-views?hl=es-419).

El usuario al que se le aplique esta pol√≠tica de permisos podr√° realizar las siguientes acciones:

- Visualizar y consultar las vistas granulares en las cuales tenga los permisos necesarios desde un proyecto de **consulta**.

**Asignaci√≥n de pol√≠tica**

Para asignar a un usuario esta pol√≠tica de permisos se deben seguir los siguientes pasos:

1. Agregar al usuario al grupo con email `analistadedatosgcpdatex@grupo-exito.com`. Este grupo tiene el rol `storage.objectViewer` asignado en el bucket `co-grupo-exito-abast-datex-trusted-data-pdn` de Cloud Storage.
2. Se debe crear un nuevo dataset en BigQuery, este dataset es el que se compartir√° con los usuarios. En este dataset se debe asignar el rol `Data Analyst Custom Rol` al grupo con email `analistadedatosgcpdatex@grupo-exito.com`.
3. En el dataset que ser√° compartido se crea la vista requerida `<nombre_vista>`. A esta vista se le debe asignar el rol `bigquery.dataViewer` al **USUARIO INDIVIDUAL** que requiere el acceso. Se debe hacer de esta forma para evitar que otros usuarios pertenecientes al grupo tengan acceso a vistas no permitidas o requeridas, dado que si se asigna este rol al grupo entero, todos los miembros tendr√≠an acceso a la totalidad de las vistas del dataset compartido.
4. Se debe autorizar la vista. Para esto vamos al dataset fuente, de donde creamos la vista, por ejemplo, `trusted_abast_instaleap_external_tables`. En el dataset fuente seguimos los siguientes pasos:
1. En el panel¬†**Explorador**, selecciona el conjunto de datos.
2. Expande la opci√≥n¬†**Acciones**¬†y haz clic en¬†**Abrir**.
3. Haz clic en¬†**Compartir**¬†y, luego, selecciona¬†**Autorizar vistas**.
4. En el panel¬†**Vistas autorizadas**¬†que se abre, ingresa la vista¬†`<nombre_vista>`¬†en el campo¬†**Vista autorizada**.
5. Haz clic en¬†**Agregar autorizaci√≥n**.
5. Si se requieren m√°s detalles sobre la autorizaci√≥n de vistas, se debe visitar la p√°gina [Crear una vista autorizada](https://cloud.google.com/bigquery/docs/share-access-views?hl=es-419).

> üìò NOTA Con esta pol√≠tica de permisos se garantiza que las identidades que utilizan la soluci√≥n como analistas de datos, no pueden acceder a los objetos almacenados en Cloud Storage desde la consola de Google. Pero se debe hacer cierta salvedad: estos S√ç podr√≠an acceder a los objetos desde la Cloud Shell de Google, si esta no est√° desactivada para los usuarios, lo que implica un riesgo en cuanto a la filtraci√≥n de datos sensibles. Este es un requisito importante, no solo para lo mencionado, si no tambi√©n para implementar soluciones en la nube con mejores pr√°cticas de seguridad. Si a√∫n no est√° deshabilitado el acceso a la Cloud Shell de Google, se debe seguir la siguiente documentaci√≥n oficial para hacerlo: Inhabilita o restablece Cloud Shell.
>

> üìò NOTA GENERAL La recomendaci√≥n por parte de Google es trabajar con ROLES PREDEFINIDOS, ya que estos son administrados por el propio Google, por lo que van a funcionar ante cualquier eventualidad. Los ROLES PERSONALIZADOS se deben usar si los PREDEFINIDOS no cumplen con los requerimientos de permisos. Hay que tener en cuenta que la administraci√≥n de estos roles queda a cargo de los administradores de la soluci√≥n.
>
>
> Se tienen los siguientes roles en la pol√≠tica de permisos asociada a un **Cient√≠fico de Datos**:
>
> - **Personalizado:** `Data Scientist Custom Role` a nivel de proyecto que contiene solo los permisos definidos en la pol√≠tica.
>
> Y para la pol√≠tica de **Analista de Datos**:
>
> - **Predefinido:** visualizador de datos de BigQuery (`roles/bigquery.dataViewer`) a nivel de tabla.
> - **Predefinido:** visualizador de objetos de Storage (`roles/storage.objectViewer`) a nivel de bucket.
> - **Personalizado:** `Data Analyst Custom Rol` rol a nivel de dataset que contiene solo los permisos definidos en la pol√≠tica para el dataset autorizado.

### **Cuentas de servicios**

Una cuenta de servicio es una identidad de Google que se utiliza para acceder a los recursos y servicios de Google Cloud Platform (GCP) de forma program√°tica, sin necesidad de interactuar directamente con la consola de GCP. Este enfoque program√°tico permite a las aplicaciones y servicios que se ejecutan en GCP, como contenedores, instancias de Compute Engine y App Engine, acceder a otros servicios de GCP, como BigQuery o Cloud Storage.
Es importante destacar que, al igual que con cualquier otra cuenta, las cuentas de servicio deben seguir las pol√≠ticas de seguridad de GCP, incluida la pol√≠tica de privilegios m√≠nimos, y deben asignarse roles que limiten sus permisos para controlar su interacci√≥n con otros recursos.

#### Cuentas de servicio administradas por el usuario

Las cuentas de servicio administradas por el usuario son identidades espec√≠ficas creadas y gestionadas por los usuarios para acceder y controlar recursos en la plataforma de Google Cloud. Estas cuentas est√°n dise√±adas para permitir a los usuarios asignar roles y permisos personalizados, brindando flexibilidad en la administraci√≥n de accesos y autorizaciones a servicios y recursos en la nube de Google.
La asignaci√≥n de roles se realiza en el proyecto en la secci√≥n IAM & Admin, agregando la cuenta de servicio con los roles definidos.

#### Cuentas de servicio administradas por google

Por otro lado, las cuentas de servicio administradas por Google, a veces referidas como cuentas de servicio predeterminadas, se crean autom√°ticamente en ciertos proyectos y se utilizan para ejecutar un servicio de Google espec√≠fico. Estas cuentas de servicio tienen asignados los permisos necesarios para que el servicio correspondiente funcione correctamente.
Por ejemplo, en el proyecto, existe una cuenta de servicio administrada por Google para Compute Engine, que generalmente tiene un formato similar a compute@developer.gserviceaccount.com. Esta cuenta de servicio es utilizada por las instancias de Compute Engine para interactuar con otros servicios de GCP.

As√≠, en el contexto del proyecto en cuesti√≥n se tiene lo siguiente:

| Cuenta de Servicio                                                                                  | Administrada por el usuario | Descripci√≥n                                                                                                                                                                                                                                                               |
|-----------------------------------------------------------------------------------------------------|-----------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| sa-[ambiente]-teradata-ingest@co-grupoexito-funnel-mercd-[ambiente]_gcp.iam.gserviceaccount.com     | Si                          | Una cuenta de servicio que ser√° utilizada por Instaleap para escribir datos en el bucket raw. Esta cuenta tiene asignados los roles predefinidos de Storage Object Creator y Storage Object Viewer, lo que le permite escribir y consultar datos en Google Cloud Storage. |
| sa-[ambiente]-scheduler-orchestration@co-grupoexito-funnel-mercd-[ambiente].iam.gserviceaccount.com | Si                          | Esta cuenta de servicio es utilizada por el recurso Cloud Scheduler para ejecutar un workflow en funci√≥n de una programaci√≥n. Tiene asignado un rol personalizado con los permisos dataproc.workflowTemplates.instantiate y iam.serviceAccounts.actAs                     |


Es crucial recordar que al trabajar con cuentas de servicio, ya sean creadas por el usuario o administradas por Google, deben seguirse las mejores pr√°cticas de seguridad que permitan garantizar el principio de privilegio m√≠nimo. Esto significa asignar solamente los permisos que son necesarios para el desempe√±o de las funciones que la cuenta de servicio requiere, revisar y actualizar regularmente estos permisos para mantener la seguridad y eficiencia de los recursos y servicios de GCP.



